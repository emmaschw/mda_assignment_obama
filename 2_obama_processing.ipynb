{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c225ec06",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b5da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data wrangling\n",
    "import pandas as pd\n",
    "\n",
    "# regular expressions\n",
    "import re\n",
    "\n",
    "# lemmatisation \n",
    "import spacy \n",
    "\n",
    "# n-grams \n",
    "import gensim\n",
    "from gensim.models import Phrases \n",
    "from gensim.models import phrases\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS \n",
    "\n",
    "# word counter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1587e6ce",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45028ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_excel(\"obama_speeches.xlsx\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651168a",
   "metadata": {},
   "source": [
    "## Noise Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c8134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_document_formatting(text):\n",
    "    # separate line breaks from words\n",
    "    p1 = re.compile(r'(\\n)([A-Za-z\\\\[])') \n",
    "    text = re.sub(p1, r\"\\1 \\2\", text) \n",
    "    p2 = re.compile(r'([A-Za-z])(\\n)') \n",
    "    text = re.sub(p2, r\"\\1 \\2\", text)\n",
    "\n",
    "    # remove footer\n",
    "    text = re.sub('(AAm|AmericanRhetoric\\.com)\\s((.||\\n)*?)\\sPage\\s\\d{1,2}', '', text)\n",
    "    text = re.sub('(meerriiccaannR)\\s((.||\\n)*?)\\s(Property)', '', text)\n",
    "\n",
    "    # remove everything up until (and including) the sentence with the date of the speech\n",
    "    text = re.sub(r'^((.|\\n)*)\\s(\\d{1,2}\\s{1,2}[a-zA-Z]{3,9},?\\s\\d{4},?)\\s.*\\s\\n', '', text)  \n",
    "\n",
    "    # remove everything up until (and including) the statement about transcription\n",
    "    text = re.sub('^((.|\\n)*)\\s(\\\\[?AUTHENTICITY)\\s.*\\s\\n', '', text)  \n",
    "\n",
    "    # remove line breaks\n",
    "    text = re.sub(\"\\n\", \"\", text) \n",
    "    \n",
    "    # remove multiple white spaces\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    \n",
    "    # lower case\n",
    "    text = text.lower() \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3332c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_noise = [remove_document_formatting(text) for text in df['speech']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabeebe",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16198c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisation(texts):\n",
    "    n_grams_list = []\n",
    "    for text in texts:\n",
    "        new = simple_preprocess(text, deacc = True)\n",
    "        n_grams_list.append(new)\n",
    "    return n_grams_list\n",
    "\n",
    "\n",
    "def find_n_grams(texts):\n",
    "    new_data = tokenisation(texts)\n",
    "    bigram_phrases = Phrases(new_data, min_count = 5, threshold = 120, \n",
    "                             connector_words = phrases.ENGLISH_CONNECTOR_WORDS)\n",
    "\n",
    "    trigram_phrases = Phrases(bigram_phrases[new_data], threshold = 120, \n",
    "                              connector_words = phrases.ENGLISH_CONNECTOR_WORDS)\n",
    "    \n",
    "    bigram = phrases.Phraser(bigram_phrases)\n",
    "    \n",
    "    trigram = phrases.Phraser(trigram_phrases)\n",
    "    \n",
    "    ngrams = [bigram[text] for text in new_data]\n",
    "    ngrams = [trigram[bigram[text]] for text in new_data]\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a672976",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = find_n_grams(no_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f0850",
   "metadata": {},
   "source": [
    "## Text Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb203b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalisation(texts):\n",
    "    nlp = spacy.load('en_core_web_sm', disable = ['parser', 'ner'])\n",
    "    postags = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    normalised_data = []\n",
    "\n",
    "    for word in texts:\n",
    "            doc = nlp(\" \".join(word)) \n",
    "            normalised_data.append([token.lemma_ for token in doc if token.pos_ in postags])\n",
    "    \n",
    "    return normalised_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "128bde78",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised = text_normalisation(n_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86fbab",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb8ca8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = []\n",
    "def stopword_removal(texts):\n",
    "    no_stopwords = []\n",
    "    remove_sw_from_list = {\"bill\"}\n",
    "    stopwords = STOPWORDS.difference(remove_sw_from_list)\n",
    "    stopwords = stopwords.union(set(extension))\n",
    "    \n",
    "    for text in texts:\n",
    "        # remove stopwords\n",
    "        no_stops_text = [word for word in text if word not in stopwords]\n",
    "        no_stops_text = \" \".join( no_stops_text)\n",
    "        \n",
    "        # remove punctuation \n",
    "        no_puncts = re.sub(\"[^a-zA-Z\\d\\s/\\_]\", \"\", no_stops_text)\n",
    "        \n",
    "        no_stopwords.append(no_puncts) \n",
    "    \n",
    "    return no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3599493",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stopwords = stopword_removal(normalised)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9242af",
   "metadata": {},
   "source": [
    "## Removal of Low and High Frequency Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a369fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts):\n",
    "    texts = tokenisation(texts)\n",
    "    count_words = Counter([i for sublist in texts for i in sublist])\n",
    "    return count_words\n",
    "\n",
    "def print_vocab_size(texts):\n",
    "    print(\"Total Vocabulary Size: \" + str(len(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d14567ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 14781\n"
     ]
    }
   ],
   "source": [
    "# compute vocabulary size\n",
    "count_words_pre_cleaning = count_words(no_stopwords)\n",
    "print_vocab_size(count_words_pre_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9c7d2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "      <th>perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>6275</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ve</th>\n",
       "      <td>5192</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>4081</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>3472</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>3189</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>3060</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>2977</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>2973</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>2903</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>2762</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>2659</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>2425</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need</th>\n",
       "      <td>2314</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>2262</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>2185</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         freq  perc\n",
       "people   6275  1.36\n",
       "ve       5192  1.12\n",
       "work     4081  0.88\n",
       "country  3472  0.75\n",
       "know     3189  0.69\n",
       "want     3060  0.66\n",
       "year     2977  0.64\n",
       "time     2973  0.64\n",
       "world    2903  0.63\n",
       "think    2762  0.60\n",
       "come     2659  0.58\n",
       "right    2425  0.53\n",
       "need     2314  0.50\n",
       "new      2262  0.49\n",
       "help     2185  0.47"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute word frequencies\n",
    "dict_df = pd.DataFrame.from_dict(count_words_pre_cleaning, orient = 'index', columns = ['freq'])\n",
    "dict_df['perc'] = (dict_df['freq'] / dict_df['freq'].sum()) * 100\n",
    "dict_df.sort_values('perc', ascending = False).head(15).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "401fb3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 3198\n"
     ]
    }
   ],
   "source": [
    "# add low and high frequency words to extension list\n",
    "min_freq = 15; max_freq = 1800\n",
    "extension = dict_df[(dict_df.freq <= min_freq) | (dict_df.freq >= max_freq)].index.tolist()\n",
    "\n",
    "# remove stopwords\n",
    "final_clean = stopword_removal(normalised)\n",
    "\n",
    "# compute vocabulary size\n",
    "count_words_post_cleaning = count_words(final_clean)\n",
    "print_vocab_size(count_words_post_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "119639ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'behalf great state crossroad land let express deep gratitude privilege address convention tonight particular honor let face presence stage pretty unlikely father foreign student bear raise small village grow school father grandfather cook domestic british grandfather large dream son hard perseverance father scholarship study place shine beacon freedom opportunity study father meet mother bear town father oil rig farm depression day grandfather sign duty join march home grandmother raise baby line war study bill buy later west search opportunity big dream daughter common dream bear continent parent share love share abide faith possibility african barack bless believe barrier success imagine imagine school land weren rich generous rich achieve potential pass night look great pride stand stand grateful diversity heritage aware parent dream live precious daughter stand story large story owe debt earth story possible tonight gather affirm greatness height power military size economy pride base simple premise sum declaration years_ago hold truth man created_equal endow creator_with_certain inalienable_right life liberty_and_the_pursuit_of_happiness true faith simple dream insistence small child night feed safe harm write hear sudden knock door idea start business participate political process fear vote count election reaffirm value commitment hold hard reality measure legacy promise future generation fellow republican independent tonight worker meet lose union plant compete child pay buck hour meet lose tear wonder pay dollar month drug son health benefit count young woman thousand grade drive money college wrong meet big city office park expect government solve problem hard ahead county tell tax money waste welfare agency neighborhood folk tell government teach kid learn parent teach child achieve raise expectation turn television set eradicate black youth book act white thing expect expect government solve problem sense deep bone change priority sure child decent shot life doors_of_opportunity remain open choice election offer choice party choose man lead embody offer man understand ideal community faith service define life heroic service prosecutor governor decade devote tough_choice easy available value record affirm believe hard reward instead offer tax_break company ship overseas offer company create home believe afford health coverage politician believe energy independence aren profit oil_companie foreign_oil field believe constitutional freedom sacrifice basic liberty use faith divide believe dangerous war option option meet young man look kid easy smile tell join marine head week listen explain enlist absolute faith leader duty service young man hope child ask serve serve men_and_woman sons_and_daughter husband wife friend neighbor win return hometown family meet struggle love income return miss shatter lack health benefit send young men_and_woman harm solemn_obligation number truth care family tend soldier return war troop win war secure peace earn respect let clear let clear real enemy enemy pursue defeat hesitate risk life protect man serve hesitate moment use military safe secure belief prosper famous ingredient belief connect child read matter child senior citizen pay prescription_drug choose medicine life poor grandparent arab family round benefit threaten civil_libertie fundamental belief fundamental belief my_brother_keeper allow pursue individual dream family speak prepare divide master negative embrace politic tonight conservative black like democrat news worship awesome god federal agent library coach little gay friend patriot oppose war patriot support war pledge allegiance star defend end end end election participate politic cynicism participate politic hope hope hope talk ignorance unemployment away crisis solve ignore talk talk substantial hope slave sit freedom song hope immigrant set distant shore hope young naval patrol hope son defy odd hope kid believe place hope hope face difficulty hope face uncertainty hope end great gift belief thing belief day ahead believe middle_class relief provide family road opportunity believe provide home homeless reclaim young city violence despair believe righteous wind stand crossroad history choice meet challenge face tonight feel energy feel urgency feel passion feel no_doubt rise swear swear reclaim promise long political darkness bright day everybody_god_bless'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ebb6c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data as txt file\n",
    "with open(\"clean_obama_speeches.txt\", \"w\") as file:\n",
    "    for text in final_clean:\n",
    "        file.write(\"%s\\n\" % text) # add line break after each speech to keep speeches separate\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e549d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
