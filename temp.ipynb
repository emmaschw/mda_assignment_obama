{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb5403d8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb6970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code chunk can be removed in the final notebook since the requirements.txt file lists all used packages!\n",
    "!pip install requests openpyxl PyMuPDF glob2 nltk spacy pandas gensim Counter matplotlib seaborn wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696bd5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c2d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing APIs and URLs\n",
    "import requests\n",
    "\n",
    "# static web scraping\n",
    "from urllib.request import urlopen\n",
    "from lxml.html import parse, fromstring\n",
    "\n",
    "# regular expressions\n",
    "import re\n",
    "\n",
    "# downloading files\n",
    "import urllib.request\n",
    "\n",
    "# operating system\n",
    "import os\n",
    "\n",
    "# looping through folder\n",
    "import glob\n",
    "\n",
    "# reading pdfs\n",
    "import fitz \n",
    "\n",
    "# disabling warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nlp\n",
    "import spacy\n",
    "\n",
    "# data wrangling\n",
    "import pandas as pd\n",
    "\n",
    "# topic modelling\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# counting words\n",
    "from collections import Counter\n",
    "\n",
    "# data viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# grahps\n",
    "import seaborn as sns\n",
    "\n",
    "# word cloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# centering plots\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6eb6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# center plots\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e336c7",
   "metadata": {},
   "source": [
    "# Data Collection and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe0671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all links from website\n",
    "tree = parse(urlopen(\"https://www.americanrhetoric.com/barackobamaspeeches.htm\"))\n",
    "linkelements = tree.xpath(\"//a\")\n",
    "list_links = [e.attrib[\"href\"] for e in linkelements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f83fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of links: \" + str(len(list_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11274b3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# only retain pdf links\n",
    "p = re.compile('.*pdf$')\n",
    "pdf_links = [ s for s in list_links if p.match(s) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e8227",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of pdf links: \" + str(len(pdf_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concetenate baseurl and path\n",
    "baseurl = \"https://www.americanrhetoric.com/\"\n",
    "full_pdf_links = [baseurl + link for link in pdf_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ab6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new folder\n",
    "folder_name = \"obama_speeches\"\n",
    "os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc20e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all pdf files\n",
    "def download_files(links, folder):\n",
    "    i = 1\n",
    "    for link in links:\n",
    "        x = folder + \"/file_\" + str(i) + \".pdf\"\n",
    "        urllib.request.urlretrieve(link, x)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f86538",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_files(full_pdf_links, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import files\n",
    "def import_pdfs(folder):\n",
    "    \n",
    "    # sort pdf files by name\n",
    "    numbers = re.compile(r'(\\d+)')\n",
    "    def numericalSort(value):\n",
    "        parts = numbers.split(value)\n",
    "        parts[1::2] = map(int, parts[1::2])\n",
    "        return parts\n",
    "    filename_list = sorted(glob.glob(folder + \"/*.pdf\"), key = numericalSort)\n",
    "    \n",
    "    # create empty list\n",
    "    speech = []\n",
    "    \n",
    "    # loop through all files\n",
    "    for filename in filename_list:\n",
    "        with fitz.open(filename) as doc:\n",
    "            text = \"\"\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "        speech.append(text)\n",
    "        \n",
    "    return speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e65b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_speeches = import_pdfs(folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73bf8ab",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a8a72a",
   "metadata": {},
   "source": [
    "## Noise Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b234af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise removal and standardisation\n",
    "def noise_removal(texts):\n",
    "    no_noise = []\n",
    "    for text in texts:\n",
    "        \n",
    "        # separate line breaks from words\n",
    "        p1 = re.compile(r'(\\n)([A-Za-z\\\\[])') \n",
    "        text = re.sub(p1, r\"\\1 \\2\", text) \n",
    "        p2 = re.compile(r'([A-Za-z])(\\n)') \n",
    "        text = re.sub(p2, r\"\\1 \\2\", text)\n",
    "        \n",
    "        # remove footer\n",
    "        text = re.sub('(AAm|AmericanRhetoric\\.com)\\s((.||\\n)*?)\\sPage\\s\\d{1,2}', '', text)\n",
    "        text = re.sub('(meerriiccaannR)\\s((.||\\n)*?)\\s(Property)', '', text)\n",
    "        \n",
    "        # remove everything up until (and including) the sentence with the date of the speech\n",
    "        text = re.sub(r'^((.|\\n)*)\\s(\\d{1,2}\\s{1,2}[a-zA-Z]{3,9},?\\s\\d{4},?)\\s.*\\s\\n', '', text)  \n",
    "        \n",
    "        # remove everything up until (and including) the statement about transcription\n",
    "        text = re.sub('^((.|\\n)*)\\s(\\\\[?AUTHENTICITY)\\s.*\\s\\n', '', text)  \n",
    "        \n",
    "        # remove line breaks\n",
    "        text = re.sub(\"\\n\", \"\", text) \n",
    "        \n",
    "        # remove multiple white spaces\n",
    "        text = re.sub(\"\\s+\", \" \", text) \n",
    "        \n",
    "        # lower case\n",
    "        text = text.lower() \n",
    "        \n",
    "        # remove punctuation and most special characters\n",
    "        text = re.sub(\"[^a-zA-Z\\d\\s/]\", \"\", text)\n",
    "        no_noise.append(text)\n",
    "        \n",
    "    return no_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aba4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_noise = noise_removal(list_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword removal\n",
    "stop_words = stopwords.words(\"english\")\n",
    "no_stopwords = [[word for word in word_tokenize(text) if word not in stop_words] for text in no_noise]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf58ba7",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b4f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create n-grams\n",
    "def n_grams(texts, min_count, threshold):\n",
    "    \n",
    "    # setup\n",
    "    bigram = gensim.models.Phrases(texts, min_count = min_count, threshold = threshold)\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold = threshold)  \n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # add bigrams\n",
    "    n_grams = [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    # add trigrams\n",
    "    n_grams = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4039e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_trigrams = n_grams(no_stopwords, 5, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6372cf",
   "metadata": {},
   "source": [
    "## Text Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ccb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text normalisation with pos tags\n",
    "def text_normalisation(texts):\n",
    "    nlp = spacy.load('en_core_web_sm', disable = ['parser', 'ner'])\n",
    "    postags = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    normalised_data = []\n",
    "\n",
    "    for word in texts:\n",
    "            doc = nlp(\" \".join(word)) \n",
    "            normalised_data.append([token.lemma_ for token in doc if token.pos_ in postags])\n",
    "    \n",
    "    return normalised_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974501bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_data = text_normalisation(bigrams_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e260f73",
   "metadata": {},
   "source": [
    "## Removal of Low and High Frequency Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute vocabulary size\n",
    "def count_words(texts):\n",
    "    count_words = Counter([i for sublist in texts for i in sublist])\n",
    "    return count_words\n",
    "\n",
    "def print_vocab_size(texts):\n",
    "    print(\"Total Vocabulary Size: \" + str(len(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5bc718",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words_pre_cleaning = count_words(normalised_data)\n",
    "print_vocab_size(count_words_pre_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c54ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word frequencies\n",
    "dict_df = pd.DataFrame.from_dict(count_words_pre_cleaning, orient = 'index', columns = ['freq'])\n",
    "dict_df['perc'] = (dict_df['freq'] / dict_df['freq'].sum()) * 100\n",
    "dict_df.sort_values('perc', ascending = False).head(15).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f03e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add low and high frequency words to stop_words\n",
    "min_freq = 20; max_freq = 2300\n",
    "extension = dict_df[(dict_df.freq <= min_freq) | (dict_df.freq >= max_freq)].index.tolist()\n",
    "extension = extension + [\"lot\", \"thing\", \"let\", \"use\", \"sure\", \"look\", \"tell\", \"many\", \"much\", \"thank\"]\n",
    "stop_words.extend(extension)\n",
    "\n",
    "# remove stopwords\n",
    "stop_words_extended = set(stop_words)\n",
    "cleaned_data = [[token for token in text if token not in stop_words_extended] for text in normalised_data]\n",
    "\n",
    "# compute vocabulary size\n",
    "count_words_post_cleaning = count_words(cleaned_data)\n",
    "print_vocab_size(count_words_post_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3732c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create term document frequency\n",
    "id2word = corpora.Dictionary(cleaned_data)\n",
    "corpus = [id2word.doc2bow(text) for text in cleaned_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e8253",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d423df",
   "metadata": {},
   "source": [
    "## Determining the Number of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0476b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'elbow' method\n",
    "def coherence_values(corpus, id2word, start, limit, step):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = id2word, num_topics = num_topics, \n",
    "                                          random_state = 100, update_every = 1, chunksize = 100, \n",
    "                                          passes = 10, alpha = 'auto', per_word_topics = True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model = model, texts = cleaned_data, dictionary = id2word, \n",
    "                                        coherence = 'c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08372755",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 2; limit = 20; step = 2\n",
    "coherence_values = coherence_values(corpus, id2word, start, limit, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.rcParams['figure.figsize'] = [14, 7]\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values, label = \"Coherence Values\")\n",
    "plt.xlabel(\"Number of Topics\", fontsize = 20)\n",
    "plt.ylabel(\"Coherence Values\", fontsize = 20)\n",
    "plt.legend(loc=\"upper right\", fontsize = 15)\n",
    "plt.title('Elbow Method', fontsize = 25, pad = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e5e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine coherence values\n",
    "for number_topics, cv in zip(x, coherence_values):\n",
    "    print(number_topics, 'topics have a coherence value of', round(cv, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee3b97",
   "metadata": {},
   "source": [
    "Based on the coherence values, 8 topics seem to be the best choice for our data. However, a coherence value of 0.462 is rather low, so maybe more pre-processing is needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3398e7d",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab40161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build topic model\n",
    "number_topics = 8\n",
    "topic_model_lda = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = id2word, num_topics = number_topics, \n",
    "                                                  random_state = 100, update_every = 1, chunksize = 100, \n",
    "                                                  passes = 14, alpha = 'auto', per_word_topics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity\n",
    "print('Perplexity: ', round(topic_model_lda.log_perplexity(corpus), 3))\n",
    "\n",
    "# compute coherence value\n",
    "coherence_topic_model_lda = CoherenceModel(model = topic_model_lda, texts = cleaned_data, dictionary = id2word, \n",
    "                                           coherence = 'c_v')\n",
    "coherence_topic_model_lda_values = coherence_topic_model_lda.get_coherence()\n",
    "print('Coherence Value: ', round(coherence_topic_model_lda_values, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9809f02",
   "metadata": {},
   "source": [
    "interpretation??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b59389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame of topics with corresponding keywords\n",
    "lda_topics = [[(term, round(weight, 3)) for term, weight in topic_model_lda.show_topic(n, topn = 20)] \n",
    "              for n in range(0, topic_model_lda.num_topics)]\n",
    "lda_topics_df = pd.DataFrame([', '.join([term for term, weight in topic]) \n",
    "                              for topic in lda_topics], columns = ['keywords'],\n",
    "                             index = ['topic_'+str(t) for t in range(1, topic_model_lda.num_topics + 1)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb39046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace generic index with topic names\n",
    "index_names = lda_topics_df.index.values.tolist()\n",
    "topic_names = [\"defence\", \"national pride\", \"labour market\", \"health care\", \"financial sector\", \n",
    "               \"support system\", \"future\", \"political ambitions\"]\n",
    "lda_topics_df = lda_topics_df.rename(index = dict(zip(index_names, topic_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bde017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print data frame\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "lda_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6917959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset display settings\n",
    "pd.reset_option('^display.', silent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot settings\n",
    "wc = WordCloud(background_color = \"white\", colormap = \"tab10\",\n",
    "               max_font_size = 150, random_state = 42)\n",
    "plt.rcParams['figure.figsize'] = [20, 8]\n",
    "\n",
    "# create wordcloud for each topic\n",
    "for i in range(topic_model_lda.num_topics):\n",
    "    wc.generate(text = lda_topics_df[\"keywords\"][i])\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(wc, interpolation = \"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(lda_topics_df.index[i], fontsize = 22, y = 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4647b9d",
   "metadata": {},
   "source": [
    "## Topic Distribution by Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract distribution of topics by speech\n",
    "topic_distribution_speeches = [topic_model_lda.get_document_topics(item, \n",
    "                                                                   minimum_probability = 0.0) for item in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff7908",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6088531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame of topic distributions by speech\n",
    "lda_df_proportions = pd.DataFrame.from_records([{v: k for v, k in row} for row in topic_distribution_speeches])\n",
    "lda_df_proportions.columns = topic_names\n",
    "lda_df_proportions['file'] = lda_df_proportions.reset_index().index + 1\n",
    "lda_df_proportions = lda_df_proportions.set_index('file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb53ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame of speeches\n",
    "d = {'speech': list_speeches}\n",
    "speeches_df = pd.DataFrame(d)\n",
    "\n",
    "# add file name\n",
    "speeches_df['file'] = speeches_df.reset_index().index + 1\n",
    "speeches_df = speeches_df.set_index('file')\n",
    "\n",
    "# extract dates of speeches\n",
    "date_reg = r'(\\d{1,2}\\s{1,2}[a-zA-Z]{3,9},?\\s?\\d{4}?)'\n",
    "speeches_df['date'] = speeches_df['speech'].str.extract(date_reg, expand = False)\n",
    "\n",
    "# manually fix dates that were not picked up by regex\n",
    "speeches_df.at[271, 'date'] = '2014-07-18'\n",
    "speeches_df.at[329, 'date'] = '2015-07-15'\n",
    "speeches_df.at[377, 'date'] = '2016-02-26'\n",
    "speeches_df['date'] = pd.to_datetime(speeches_df['date'])\n",
    "\n",
    "# drop speech column\n",
    "speeches_df = speeches_df.drop('speech', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae114505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge both data frames\n",
    "df_merged = pd.merge(lda_df_proportions, speeches_df, on = 'file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year of speeches\n",
    "df_merged['year'] = df_merged['date'].dt.year.convert_dtypes()\n",
    "\n",
    "# average topic distribution per year\n",
    "topic_distribution_df = df_merged.groupby('year', as_index = False)[topic_names].mean().copy()\n",
    "\n",
    "# transform data frame from wide to long format\n",
    "topic_distribution_df_melt = topic_distribution_df.melt(id_vars = 'year', value_vars = topic_names, \n",
    "                                                        var_name = 'topic', value_name = 'prevelance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57101f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot topic distribution by year\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [14, 7]\n",
    "topics_by_year = sns.lineplot(data = topic_distribution_df_melt, x = \"year\", y = \"prevelance\", hue = \"topic\",\n",
    "                              linewidth = 2.5)\n",
    "topics_by_year.set_xlabel(\"Year\", fontsize = 20)\n",
    "topics_by_year.set_ylabel(\"Prevalence\", fontsize = 20)\n",
    "plt.legend(ncol = 2, loc = 'upper center', fontsize = 13, title = \"Topics\", title_fontsize = 15, markerscale = 1.5)\n",
    "plt.title('Topic Distribution by Year', fontsize = 25, pad = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f42136",
   "metadata": {},
   "source": [
    "# Topic Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad844ca",
   "metadata": {},
   "source": [
    "## Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450711f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CAPTIONS\n",
    "\n",
    "from pprint import pprint\n",
    "from gensim.models import CoherenceModel\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as LDAgensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb25f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6bb3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We construct our LDA model\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics = 6, \n",
    "                                            random_state = 100, update_every = 1, chunksize = 100, passes = 14, alpha = 'auto', per_word_topics=True) # Here we selected 5 topics\n",
    "pprint(model.print_topics())\n",
    "model_cor = model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we calculate coherence score and perplexity\n",
    "\n",
    "model_coher = CoherenceModel(model=model, texts=cleaned_data, dictionary=id2word, coherence='c_v')\n",
    "coher_s = model_coher.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)\n",
    "print('Perplexity: ', model.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239af5c2",
   "metadata": {},
   "source": [
    "To find the most important words for each topic, we first find the dominant topics by taking the distribution of the topics per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aff3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist = [model.get_document_topics(item, minimum_probability=0.0) for item in corpus]\n",
    "topic_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8a1966",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cor = [sorted(topics, key=lambda record: -record[1])[0] for topics in topic_dist]\n",
    "top_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0dedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [[(term, round(wt, 3)) for term, wt in model.show_topic(n, topn=20)] for n in range(0, model.num_topics)]\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9d574c",
   "metadata": {},
   "source": [
    "Next, we construct a dataframe matrix for the topics and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd08b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_mat = pd.DataFrame([[term for term, wt in topic] for topic in topics], columns = ['Keyword '+str(i) for i in range(1, 21)],\n",
    "                         index=['Topic '+str(t) for t in range(1, model.num_topics+1)]).T\n",
    "topics_mat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c06de",
   "metadata": {},
   "source": [
    "The keywords per topic are now viewable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24875538",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "topics_mat = pd.DataFrame([', '.join([term for term, wt in topic]) for topic in topics], columns = ['Topic Keywords'],\n",
    "                         index=['Topic'+str(t) for t in range(1, model.num_topics+1)] )\n",
    "topics_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2d0d6",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f612a",
   "metadata": {},
   "source": [
    "We construct a wordcloud for our LDA model from the keywords for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eafab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42)\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "# We also construct subplots per topic\n",
    "for i in range(model.num_topics): # this is how many topics we show the wordclouds for\n",
    "\n",
    "    cloud.generate(text=topics_mat[\"Topic Keywords\"][i])\n",
    "    \n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(topics_mat.index[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069ee61",
   "metadata": {},
   "source": [
    "### Word count and word weights or significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88615d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create the data frame for the word count and keyword weights \n",
    "tops = model.show_topics(formatted=False)\n",
    "flat_data = [w for w_list in cleaned_data for w in w_list]\n",
    "counts = Counter(flat_data)\n",
    "\n",
    "output = []\n",
    "for i, topic in tops:\n",
    "    for word, weight in topic:\n",
    "        output.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'weights', 'word_count'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we plot the word count and the keyword weights\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16,10), sharey=True, dpi=160)\n",
    "colors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=colors[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    axtwin = ax.twinx()\n",
    "    axtwin.bar(x='word', height=\"weights\", data=df.loc[df.topic_id==i, :], color=colors[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=colors[i])\n",
    "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "    ax.set_title('Topic: ' + str(i), color=colors[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); axtwin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Weights of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3615455",
   "metadata": {},
   "source": [
    "### We investigate the number of speeches corresponding to a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speeches_per_topic (model, corpus, start=0, end=1):\n",
    "    full_corpus = corpus[start:end]\n",
    "    domtopics = []\n",
    "    percentage_topic = []\n",
    "    for i, corp in enumerate(full_corpus):\n",
    "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
    "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
    "        dominant_topics.append((i, dominant_topic))\n",
    "        percentage_topic.append(topic_percs)\n",
    "    return(dominant_topics, percentage_topic)\n",
    "\n",
    "domtopics, percentage_topic = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n",
    "\n",
    "# Dominant Topics per speech\n",
    "df = pd.DataFrame(domtopics, columns=['Document_Id', 'Dominant_Topic'])\n",
    "speech_dom_top = df.groupby('Dominant_Topic').size()\n",
    "df_speech_dom_top = speech_dom_top.to_frame(name='count').reset_index()\n",
    "\n",
    "# Distribution of topics by weight\n",
    "doc_weight = pd.DataFrame([dict(t) for t in percentage_topic])\n",
    "df_doc_weight = doc_weight.sum().to_frame(name='count').reset_index()\n",
    "\n",
    "# 3 main keywords per topic\n",
    "keywords3 = [(i, topic) for i, topics in model.show_topics(formatted=False) \n",
    "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "stacked_df_keywords3 = pd.DataFrame(keywords3, columns=['topic_id', 'words'])\n",
    "df_keywords3 = stacked_df_keywords3.groupby('topic_id').agg(', \\n'.join)\n",
    "df_keywords3.reset_index(level=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c44c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot speeches per dominant topic\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), dpi=120, sharey=True)\n",
    "\n",
    "# Dominant topic distribution\n",
    "ax1.bar(x='Dominant_Topic', height='count', data=df_speech_dom_top, width=.5, color='firebrick')\n",
    "ax1.set_xticks(range(df_speech_dom_top.Dominant_Topic.unique().__len__()))\n",
    "formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_keywords3.loc[df_keywords3.topic_id==x, 'words'].values[0])\n",
    "ax1.xaxis.set_major_formatter(formatter)\n",
    "ax1.set_title('Speeches by dominant topic', fontdict=dict(size=10))\n",
    "ax1.set_ylabel('Speeches')\n",
    "ax1.set_ylim(0, 1000)\n",
    "\n",
    "# Topic weights distribution\n",
    "ax2.bar(x='index', height='count', data=df_doc_weight, width=.5, color='steelblue')\n",
    "ax2.set_xticks(range(df_doc_weight.index.unique().__len__()))\n",
    "ax2.xaxis.set_major_formatter(formatter)\n",
    "ax2.set_title('Speeches by topic weights', fontdict=dict(size=10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea567bf",
   "metadata": {},
   "source": [
    "### LDA interactive visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a090ad",
   "metadata": {},
   "source": [
    "Shows the topics and their keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad93be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "interactive = gensimvis.prepare(model, corpus, id2word)\n",
    "interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720dd3c",
   "metadata": {},
   "source": [
    "### Speech dominant topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede293a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cor = pd.DataFrame()\n",
    "df_cor['Dominant Topic'] = [item[0]+1 for item in df_cor]\n",
    "df_cor['Contribution %'] = [round(item[1]*100, 2) for item in df_cor]\n",
    "df_cor['Topic Terms'] = [lda_topics_df.iloc[t[0]]['Key_Words_per_Topic'] for t in df_cor]\n",
    "\n",
    "df_cor.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7afcbd",
   "metadata": {},
   "source": [
    "### Percentages of dominant topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97958c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_df = df_cor.groupby('Dominant Topic').agg(\n",
    "                                  Doc_Count = ('Dominant Topic', np.size),\n",
    "                                  Total_Docs_Perc = ('Dominant Topic', np.size)).reset_index()\n",
    "\n",
    "dom_df['Total speech %'] = dom_df['Total speech %'].apply(lambda row: round((row*100) / len(corpus), 2))\n",
    "\n",
    "dom_df.sort_values('Total speech %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7205a0e",
   "metadata": {},
   "source": [
    "### LDA weights by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.DataFrame.from_records([{v: k for v, k in row} for row in topic_dist])\n",
    "weights.columns = ['Topic ' + str(i) for i in range(1,8)]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0112466",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = speech.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c04438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['year'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights['year'] = df2.year.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f919b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd0c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights['prevalent'] = weights.drop('year', axis=1).idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34bddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e981959",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.groupby('year')['prevalent'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dominance = weights.groupby('year')['prevalent'].value_counts(normalize=True).unstack().fillna(0)\n",
    "weight_dominance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fac578",
   "metadata": {},
   "source": [
    "### LDA Topic Distribution from 2004 to 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f1514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dom_year = weights.groupby('year')['prevalent'].value_counts(normalize=True).unstack().fillna(0).reset_index().copy()\n",
    "weight_dom_year.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d092932",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_melted_year = weight_dom_year.melt(id_vars= 'year' , value_vars=['Topic ' + str(i) for i in [1,2, 3, 4, 5, 6]], var_name='Topic', value_name='prevelance')\n",
    "weight_melted_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456dd6f7",
   "metadata": {},
   "source": [
    "### ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf6b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_melted_year.to_excel(\"/Users/fazek/OneDrive/Asztali gép/mda 2022/topic_trend.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eeb1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "_ = sns.lineplot(data=df_melted_year, x=\"year\", y=\"prevelance\", hue=\"Topic\", style=\"Topic\", palette='Dark2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16101749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d266ffc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572cc9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
